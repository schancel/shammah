// Common model utilities and types
// Phase 4: Candle removed, ONNX only

use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::path::Path;

/// Common model configuration (for custom transformers)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelConfig {
    pub vocab_size: usize,
    pub hidden_dim: usize,
    pub num_layers: usize,
    pub num_heads: usize,
    pub max_seq_len: usize,
    pub dropout: f64,
    pub device_preference: DevicePreference,
}

/// Generator configuration - supports both custom and pre-trained models
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum GeneratorConfig {
    /// Random initialization (existing behavior)
    RandomInit(ModelConfig),

    /// Pre-trained model using unified loader (generic across families/backends)
    Pretrained(crate::models::unified_loader::ModelLoadConfig),
}

impl Default for ModelConfig {
    fn default() -> Self {
        Self {
            vocab_size: 50_000,
            hidden_dim: 768,
            num_layers: 6,
            num_heads: 12,
            max_seq_len: 512,
            dropout: 0.1,
            device_preference: DevicePreference::Auto,
        }
    }
}

impl ModelConfig {
    /// Create config optimized for Apple Silicon
    pub fn for_apple_silicon() -> Self {
        Self {
            vocab_size: 50_000,
            hidden_dim: 768,
            num_layers: 6,
            num_heads: 12,
            max_seq_len: 512,
            dropout: 0.1,
            device_preference: DevicePreference::Metal,
        }
    }

    /// Create small config for fast testing (works well on CPU)
    pub fn small() -> Self {
        Self {
            vocab_size: 5000,
            hidden_dim: 128,
            num_layers: 2,
            num_heads: 4,
            max_seq_len: 256,
            dropout: 0.0,
            device_preference: DevicePreference::Auto,
        }
    }
}

/// Device configuration options (DEPRECATED: Phase 4 - kept for compatibility)
///
/// With ONNX Runtime, device selection is handled by execution providers:
/// - CoreML (Apple Neural Engine)
/// - CPU (fallback)
/// - CUDA/TensorRT (NVIDIA GPUs)
/// - DirectML (Windows GPUs)
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[deprecated(note = "Use ONNX Runtime execution providers instead")]
pub enum DevicePreference {
    /// Use best available device
    Auto,
    /// Force CPU usage
    Cpu,
    /// Force Metal (Apple Silicon GPU)
    Metal,
}

impl Default for DevicePreference {
    fn default() -> Self {
        Self::Auto
    }
}

// Phase 4: Device functions removed (Candle-based)
// ONNX Runtime handles device selection via execution providers

/// Stub: Device selection removed (Phase 4)
#[deprecated(note = "Device selection removed - use ONNX Runtime execution providers")]
pub fn get_device_with_preference(_preference: DevicePreference) -> Result<()> {
    anyhow::bail!(
        "get_device_with_preference removed in Phase 4.\n\
         ONNX Runtime handles device selection automatically via execution providers."
    )
}

/// Stub: Device info removed (Phase 4)
#[deprecated(note = "Device info removed - ONNX Runtime manages devices")]
pub fn device_info() -> String {
    "ONNX Runtime (device managed automatically)".to_string()
}

/// Stub: Metal availability check removed (Phase 4)
#[deprecated(note = "Metal check removed - ONNX Runtime handles CoreML EP")]
pub fn is_metal_available() -> bool {
    // Assume true on macOS for compatibility
    cfg!(target_os = "macos")
}

/// Model persistence
pub trait Saveable {
    fn save(&self, path: &Path) -> Result<()>;
    fn load(path: &Path) -> Result<Self>
    where
        Self: Sized;
}
