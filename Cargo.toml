[package]
name = "shammah"
version = "0.2.2"
edition = "2021"
authors = ["Shammah"]
description = "Local-first Constitutional AI proxy"
license = "MIT OR Apache-2.0"
repository = "https://github.com/shammah/claude-proxy"

[dependencies]
# CLI
clap = { version = "4.4", features = ["derive"] }
crossterm = "0.28"
ctrlc = "3.4"
rustyline = "13.0"

# TUI (Phase 2: Terminal UI refactor)
ratatui = "0.28"
ansi-to-tui = "6.0"
tui-textarea = "0.6"  # Text area widget for ratatui (TUI mode)

# Async runtime
tokio = { version = "1.35", features = ["full"] }
tokio-util = "0.7"
futures = "0.3"

# Global state for TUI output system (Phase 3.5)
once_cell = "1.19"

# HTTP client for Claude API
reqwest = { version = "0.11", default-features = false, features = ["json", "stream", "rustls-tls"] }

# HTTP server
axum = "0.7"
tower = "0.4"
tower-http = { version = "0.5", features = ["trace", "cors"] }

# Session management
dashmap = "5.5"

# Metrics
prometheus = "0.13"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Configuration
config = "0.14"
dirs = "5.0"
toml = "0.8"

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "fmt"] }
tracing-log = "0.2"  # Bridge log crate â†’ tracing (Phase 3.5)

# Time and hashing
chrono = { version = "0.4", features = ["serde"] }
sha2 = "0.10"
uuid = { version = "1.0", features = ["v4", "serde"] }

# Text processing for TF-IDF
rust-stemmers = "1.2"

# Machine Learning (Dual providers: ONNX Runtime + Candle)
# ONNX Runtime (recommended for most users)
ort = { version = "2.0.0-rc.11", features = ["download-binaries", "ndarray"] }
ndarray = "0.17"  # Multi-dimensional arrays for ONNX tensor creation
# Candle (alternative provider, optional)
candle-core = { version = "0.8", optional = true }
candle-nn = { version = "0.8", optional = true }
candle-transformers = { version = "0.8", optional = true }
# Shared dependencies
sysinfo = "0.32"  # System RAM detection for model selection
tokenizers = "0.21"  # Tokenization (used by both ONNX and Candle)
hf-hub = { version = "0.4", default-features = false, features = ["tokio", "ureq", "rustls-tls"] }  # HuggingFace Hub integration
indicatif = "0.17"  # Progress bars for download tracking
rand = "0.8"  # Random number generation for sampling
safetensors = "0.4"  # Model weight loading (used by Candle)

# CoreML/Metal support (macOS only)
[target.'cfg(target_os = "macos")'.dependencies]
# ONNX Runtime uses CoreML execution provider
# Candle uses Metal (requires candle feature flag)
candle-metal-kernels = { version = "0.8", optional = true }

# For web tools (Phase 3)
scraper = "0.18"

# Tools
async-trait = "0.1"
regex = "1.10"
glob = "0.3"
walkdir = "2.4"
fs2 = "0.4"  # File locking for concurrent weight updates

# Unix signal handling (for daemon process checks)
[target.'cfg(unix)'.dependencies]
nix = { version = "0.29", features = ["signal"] }

[dev-dependencies]
mockito = "1.2"
tempfile = "3.8"

[features]
default = ["onnx"]  # ONNX Runtime is the default provider
onnx = []  # ONNX Runtime support (always available)
candle = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers"]  # Candle support (optional)
candle-metal = ["candle", "dep:candle-metal-kernels"]  # Candle with Metal acceleration (macOS only)
cuda = []  # CUDA support (requires CUDA toolkit)
all-providers = ["onnx", "candle"]  # Both inference providers

[[bin]]
name = "shammah"
path = "src/main.rs"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
