# Shammah - Project Status

**Last Updated:** 2026-02-11
**Version:** 0.3.0 (Local Model Generation + Daemon Architecture)

## Current State: Production-Ready Local AI Proxy âœ…

Shammah is now a fully functional local-first AI coding assistant with ONNX Runtime inference, multi-turn tool execution, daemon architecture, and LoRA fine-tuning infrastructure.

---

## Completed Work

### âœ… Core Infrastructure (Phases 1-8: Complete)

**ONNX Runtime Integration:**
- Full ONNX Runtime integration with KV cache support
- Model loading from onnx-community HuggingFace repos
- Autoregressive generation with 56+ dynamic KV inputs (28 layers Ã— 2)
- Empty cache initialization and reuse across generation steps
- Supports Qwen 1.5B/3B/7B/14B models
- Output cleaning for production-quality responses

**Local Model Generation:**
- Pre-trained model support (works well from day 1)
- Adaptive model selection based on system RAM
- Progressive bootstrap (instant startup, background loading)
- Metal acceleration on Apple Silicon
- CPU fallback for maximum compatibility
- HuggingFace Hub integration with progress bars

**Multi-Turn Tool Execution:**
- 6 working tools: Read, Glob, Grep, WebFetch, Bash, Restart
- Local model tool use (XML + JSON format)
- Tool call parser (regex-based extraction)
- QwenAdapter preserves tool XML markers
- Integration with permission system
- Maximum 5 tool execution turns

**Daemon Architecture:**
- Auto-spawning daemon with PID management
- OpenAI-compatible HTTP API
- Tool pass-through (execute on client side)
- Session management with automatic cleanup
- Concurrent client support
- Prometheus metrics endpoint

**Router System:**
- Adaptive routing (tries local by default)
- Graceful degradation to teacher APIs
- Crisis detection for safety
- Model readiness checks
- Generic terminology (local/teacher)

**Multi-Provider Teacher Support:**
- Claude (Anthropic) - primary fallback
- GPT-4 (OpenAI)
- Gemini (Google)
- Grok (xAI)
- Provider-specific adapters with capability mapping

**LoRA Fine-Tuning Infrastructure:**
- WeightedExample serialization (JSONL export)
- TrainingCoordinator with JSONL queue writer
- LoRATrainingSubprocess (Python training, non-blocking)
- Weighted feedback support (10x/3x/1x)
- Python training script with weighted sampling
- 5/5 integration tests passing

**TUI System:**
- Professional terminal UI with scrollback
- Dual-layer rendering (scrollback + inline viewport)
- Shadow buffer for diff-based updates
- Streaming response support
- Rate limiting (20 FPS max)
- Proper ANSI code handling

---

## TODO List

### Phase 1: Polish & UX (High Priority)

**Textarea Improvements:**
- [ ] Add shift-return support for multi-line input expansion
- [ ] Add history navigation (up/down arrows for previous queries)
  - Rationale: Basic UX feature users expect from REPL interfaces
  - Complexity: Medium (need to track command history, handle multi-line)
  - Files: `src/cli/tui/input_widget.rs`

**Status Bar Enhancements:**
- [ ] Display actual stats in status bar (tokens, latency, model info)
  - Current: Status bar exists but doesn't show live stats
  - Need: Real-time token count, generation speed, current model
  - Files: `src/cli/tui/status_widget.rs`

**Daemon Management:**
- [ ] Add `shammah daemon stop` subcommand
  - Should kill daemon gracefully and remove PID file
  - Files: `src/cli/commands.rs`, `src/daemon/lifecycle.rs`
- [ ] Add `shammah daemon start` subcommand
  - Currently daemon auto-spawns, but explicit start is useful
  - Files: `src/cli/commands.rs`
- [ ] Add `shammah daemon status` subcommand
  - Show daemon running/stopped, PID, uptime, active sessions
  - Files: `src/cli/commands.rs`

### Phase 2: Setup & Configuration (Medium Priority)

**Setup Wizard Enhancements:**
- [ ] Support adding multiple teacher providers in wizard
  - Current: Only Claude setup during first run
  - Need: Interactive prompts for GPT-4, Gemini, Grok
  - Files: `src/cli/setup.rs`
- [ ] Support configuring multiple local models in wizard
  - Current: Auto-selects single model based on RAM
  - Need: Allow user to choose preferred models
  - Files: `src/cli/setup.rs`

**Model Adapter Support:**
- [ ] Test/verify Mistral model support with LlamaAdapter
  - Current: Llama adapter exists, should work for Mistral
  - Need: Integration test with Mistral ONNX models
  - Files: `src/models/adapters/llama.rs`
- [ ] Add adapters for other model families (Phi, DeepSeek, etc.)
  - Create model-specific adapters
  - Handle tokenizer differences
  - Files: `src/models/adapters/`

### Phase 3: Advanced Features (Lower Priority)

**Plan Mode Redesign:**
- [ ] Redesign plan mode to match Claude Code's implementation
  - Current: Basic plan mode exists but "nearly useless" (user feedback)
  - Need to research Claude Code's plan mode behavior
  - Multi-step planning with approval workflow
  - Step-by-step execution tracking
  - Files: `src/cli/plan_mode.rs` (may need major refactor)

**LoRA Training Integration:**
- [ ] Install Python dependencies for LoRA training
  - scripts/requirements.txt exists
  - Need: Virtual environment setup, dependency installation
  - Files: `scripts/train_lora.py`, `scripts/requirements.txt`
- [ ] Implement adapter loading in Rust runtime
  - Training infrastructure complete, need to load adapters
  - Files: `src/models/lora.rs`, `src/generators/qwen.rs`

### Phase 4: Documentation & Cleanup (Ongoing)

- [x] Clean up obsolete docs (moved to docs/archive/)
- [x] Update STATUS.md with current capabilities
- [ ] Update CLAUDE.md with accurate ONNX architecture
  - Replace references to Candle with ONNX Runtime
  - Update component descriptions
  - Fix architecture diagrams
- [ ] Create user guide for setup and usage
  - New file: `docs/USER_GUIDE.md`
  - Cover setup wizard, basic usage, tool execution
- [ ] Update ARCHITECTURE.md with daemon mode
  - Document client/daemon split
  - Tool pass-through architecture
  - Session management

---

## Known Issues

### 1. Plan Mode Needs Redesign
**Issue:** Current plan mode is basic and not user-friendly
**Impact:** Users find it "nearly useless" compared to Claude Code
**Fix:** Phase 3 - Study Claude Code's plan mode and redesign

### 2. Status Bar Empty
**Issue:** Status bar exists but doesn't show live stats
**Impact:** Users don't see token counts, speed, model info
**Fix:** Phase 1 - Wire up OutputManager stats to status bar

### 3. Setup Wizard Limited
**Issue:** Only supports Claude setup during first run
**Impact:** Users manually edit config to add other providers
**Fix:** Phase 2 - Add interactive prompts for all providers

### 4. LoRA Training Not Active
**Issue:** Infrastructure complete but Python deps not installed
**Impact:** Weighted training not functional yet
**Fix:** Phase 3 - Install dependencies, test end-to-end

---

## Performance Metrics

**Current Behavior:**
- Local model: Handles queries when ready (ONNX Runtime)
- Graceful fallback: Routes to teacher if model not ready
- Tool execution: Multi-turn loop (max 5 iterations)
- Startup time: <100ms (instant REPL)
- Model loading: 2-3 seconds from cache

**Architecture:**
- Runtime: ONNX Runtime with KV cache
- Models: onnx-community Qwen 1.5B/3B/7B/14B
- Acceleration: Metal (Apple Silicon), CPU fallback
- Daemon: Auto-spawn, OpenAI-compatible API
- Tools: Client-side execution via pass-through

---

## Next Steps

### Immediate Priorities (This Week)
1. Implement textarea improvements (shift-return, history)
2. Wire up status bar with live stats
3. Add daemon management subcommands
4. Test Mistral model support

### Short Term (This Month)
5. Enhance setup wizard for multi-provider
6. Install Python deps for LoRA training
7. Create user guide documentation
8. Update ARCHITECTURE.md

### Medium Term (Next Quarter)
9. Redesign plan mode (match Claude Code)
10. Implement adapter loading in runtime
11. Add more model adapters (Phi, DeepSeek)
12. Multi-model switching in REPL

### Long Term (Future)
13. Quantization for lower memory usage
14. Multi-GPU support
15. Custom domain-specific tools
16. CoreML export for Neural Engine

---

## How to Use

### Basic Usage
```bash
# Interactive REPL mode (daemon auto-spawns)
shammah

> Can you read my Cargo.toml and tell me about dependencies?
```

### Daemon Mode
```bash
# Auto-spawns on first query (no manual start needed)
shammah
> /local what is 2+2?

# Or use HTTP API directly
curl -X POST http://127.0.0.1:8000/v1/messages \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen-2.5-3b", "messages": [{"role": "user", "content": "Hello!"}]}'
```

### Tool Execution
```bash
> Can you read my README.md?
ðŸ”§ Tool: Read
   File: README.md
   Status: âœ“ Success
[Shows file contents and analysis]
```

---

## Technical Debt

### Code Quality
- [ ] Remove unused Candle imports (migration to ONNX complete)
- [ ] Clean up error messages for better UX
- [ ] Add more comprehensive logging

### Testing
- [ ] Add integration tests for daemon lifecycle
- [ ] Add tests for LoRA training end-to-end
- [ ] Test multi-provider fallback scenarios
- [ ] End-to-end test for tool pass-through

### Documentation
- [x] Update STATUS.md (this file)
- [ ] Update ARCHITECTURE.md with daemon architecture
- [ ] Update CLAUDE.md with ONNX details
- [ ] Create USER_GUIDE.md
- [ ] Document LoRA training workflow

---

## Contributors

- **Human:** Shammah (project vision, requirements)
- **AI:** Claude Sonnet 4.5 (implementation, architecture)

---

## License

MIT OR Apache-2.0
