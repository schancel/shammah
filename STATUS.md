# Shammah - Project Status

**Last Updated:** 2026-02-12
**Version:** 0.4.0 (Proxy-Only Mode + Multi-Provider Setup)

## Current State: Production-Ready Local AI Proxy ‚úÖ

Shammah is now a fully functional local-first AI coding assistant with ONNX Runtime inference, multi-turn tool execution, daemon architecture, and LoRA fine-tuning infrastructure.

---

## Completed Work

### ‚úÖ Core Infrastructure (Phases 1-8: Complete)

**ONNX Runtime Integration:**
- Full ONNX Runtime integration with KV cache support
- Model loading from onnx-community HuggingFace repos
- Autoregressive generation with 56+ dynamic KV inputs (28 layers √ó 2)
- Empty cache initialization and reuse across generation steps
- Supports Qwen 1.5B/3B/7B/14B models
- Output cleaning for production-quality responses

**Local Model Generation:**
- Pre-trained model support (works well from day 1)
- Adaptive model selection based on system RAM
- Progressive bootstrap (instant startup, background loading)
- Metal acceleration on Apple Silicon
- CPU fallback for maximum compatibility
- HuggingFace Hub integration with progress bars

**Multi-Turn Tool Execution:**
- 6 working tools: Read, Glob, Grep, WebFetch, Bash, Restart
- Local model tool use (XML + JSON format)
- Tool call parser (regex-based extraction)
- QwenAdapter preserves tool XML markers
- Integration with permission system
- Maximum 5 tool execution turns

**Daemon Architecture:**
- Auto-spawning daemon with PID management
- OpenAI-compatible HTTP API
- Tool pass-through (execute on client side)
- Session management with automatic cleanup
- Concurrent client support
- Prometheus metrics endpoint

**Router System:**
- Adaptive routing (tries local by default)
- Graceful degradation to teacher APIs
- Crisis detection for safety
- Model readiness checks
- Generic terminology (local/teacher)

**Multi-Provider Teacher Support:**
- Claude (Anthropic) - primary fallback
- GPT-4 (OpenAI)
- Gemini (Google)
- Grok (xAI)
- Provider-specific adapters with capability mapping
- Setup wizard supports adding multiple providers

**Proxy-Only Mode:**
- Use Shammah without local model (like Claude Code)
- REPL + tool execution with teacher API fallback
- Setup wizard asks: "Enable local model?" (yes/no)
- Config option: `[backend] enabled = false`
- Useful for users who want tools without model overhead

**LoRA Fine-Tuning Infrastructure:**
- WeightedExample serialization (JSONL export)
- TrainingCoordinator with JSONL queue writer
- LoRATrainingSubprocess (Python training, non-blocking)
- Weighted feedback support (10x/3x/1x)
- Python training script with weighted sampling
- 5/5 integration tests passing

**TUI System:**
- Professional terminal UI with scrollback
- Dual-layer rendering (scrollback + inline viewport)
- Shadow buffer for diff-based updates
- Streaming response support
- Rate limiting (20 FPS max)
- Proper ANSI code handling

---

## TODO List (Organized by Difficulty)

**Organization:** Items sorted easiest ‚Üí hardest for efficient progress and quick wins.

**Summary:**
- 32 total items (14 original + 18 new suggestions)
- 28/32 complete (87.5%) ‚úÖ
- 1 INCOMPLETE (Auto-compaction - partial)
- 2 BLOCKED (Mistral support, LoRA adapter loading)
- 1 OPTIONAL (Additional adapters - in progress)
- Phase 1: Quick wins (5 items, 1-2h each) ‚ö° - 5/5 COMPLETE ‚úÖ
- Phase 2: Medium difficulty (8 items, 2-4h each) - 7/8 COMPLETE (auto-compaction partial)
- Phase 3: Moderate complexity (4 items, 3-6h each) - ALL COMPLETE ‚úÖ
- Phase 4: Challenging (7 items, 3-8h each) - ALL COMPLETE ‚úÖ
- Phase 5: Complex (5 items, 4-20h each) - 1/5 COMPLETE
- Phase 6: Very complex (3 items, 20-40h each) - ALL COMPLETE ‚úÖ

**New Items Added:**
1. Error message improvements
2. Documentation cleanup
3. Config validation
4. Model download progress in TUI
5. /help improvements
6. Simple feedback system
7. Crash recovery
8. Python deps helper
9. Memory monitoring
10. Color customization
11. Flexible tool approval patterns
12. Proxy-only mode (no local model)

See `docs/ROADMAP.md` for detailed implementation plans.

### Phase 1: Quick Wins ‚ö° (Easy - 1-2 hours each)

1. **[x] Daemon stop command** - ‚úÖ COMPLETE
   - Add `shammah daemon-stop` subcommand
   - Files: `src/main.rs`, `src/daemon/lifecycle.rs`
   - Effort: 1 hour (actual)

2. **[x] Daemon start command** - ‚úÖ COMPLETE
   - Add `shammah daemon-start` subcommand
   - Files: `src/main.rs`
   - Effort: 30 minutes (actual)

3. **[x] Error message improvements** (NEW) - ‚úÖ COMPLETE
   - Make error messages user-friendly with actionable suggestions
   - Examples: "Model not found" ‚Üí "Model not downloaded. Run setup wizard..."
   - Files: `src/errors.rs`, error handling sites
   - Effort: 2 hours (actual)

4. **[x] Documentation cleanup** (NEW) - ‚úÖ COMPLETE
   - Clean up completed plan files from root directory
   - Moved 44 implementation/status docs to docs/archive/
   - Reduced root from 47 MD files to 3 (CLAUDE.md, README.md, STATUS.md)
   - Preserved all files with git history intact
   - Files: Root *.md ‚Üí docs/archive/
   - Effort: 10 minutes (actual)

5. **[x] Ctrl+/ shortcut fix** (NEW) - ‚úÖ COMPLETE
   - Added Ctrl+/ handler that sends /help command
   - Keyboard shortcut now works as advertised
   - Files: `src/cli/tui/async_input.rs`
   - Effort: 5 minutes (actual)

6. **[x] Persistent tool patterns not matching** (NEW) - ‚úÖ COMPLETE (Fixed 2026-02-13)
   - Issue: Persistent tool approval patterns not being saved in event loop mode
   - Root cause: Approval code was commented out in tool_execution.rs
   - Fixed: Uncommented and properly implemented approval saving
   - Files: `src/cli/repl_event/tool_execution.rs` (lines 115-164, 84-90)
   - Effort: 1 hour (actual)

### Phase 2: Medium Difficulty (2-4 hours each)

5. **[x] Shift-return multi-line support** - ‚úÖ COMPLETE
   - Textarea shift-return for multi-line input
   - Dynamic input area height (1-10 lines)
   - Shadow buffer automatically adjusts for expanded input
   - Files: `src/cli/tui/async_input.rs`, `src/cli/tui/mod.rs`
   - Effort: 1.5 hours (actual)

6. **[x] Status bar live stats** - ‚úÖ COMPLETE
   - Display tokens, latency, model info, speed in status bar
   - Updates automatically after each query completion
   - Files: `src/generators/mod.rs`, `src/cli/status_bar.rs`, `src/cli/repl_event/events.rs`
   - Effort: 2 hours (actual)

7. **[x] Config validation on startup** (NEW) - ‚úÖ COMPLETE
   - Validate config file and show helpful errors
   - Check API keys, paths, model sizes
   - Files: `src/config/settings.rs`, `src/config/loader.rs`
   - Effort: 1 hour (actual)

8. **[x] Daemon status command** - ‚úÖ COMPLETE
   - Add `shammah daemon-status` subcommand
   - Show running/stopped, PID, uptime, active sessions
   - Files: `src/main.rs`
   - Effort: 1 hour (actual)

9. **[x] Model download progress in TUI** (NEW) - ‚úÖ COMPLETE
   - Show download progress with updating log messages
   - Uses ProgressMessage with progress bar: `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 80%`
   - Updates automatically as download progresses
   - Marks complete (‚úì) or failed (‚úó) when done
   - Files: `src/models/download.rs`, `src/cli/messages/concrete.rs`
   - Effort: 1 hour (actual)

10. **[x] /help command improvements** (NEW) - ‚úÖ COMPLETE
   - Document all slash commands, keyboard shortcuts
   - Show tool confirmation system usage
   - Files: `src/cli/commands.rs`
   - Effort: 30 minutes (actual)

11. **[x] Inline ghost text suggestions** (NEW) - ‚úÖ COMPLETE
   - Claude Code-style inline autocomplete with ghost text
   - Shows command suggestions as grayed-out text after cursor
   - Tab to accept, continue typing to ignore
   - Supports slash commands and common patterns
   - Files: `src/cli/tui/mod.rs` (update_ghost_text), `src/cli/tui/async_input.rs`, `src/cli/tui/input_widget.rs`
   - Effort: 2-3 hours (actual)

12. **[~] Conversation auto-compaction** (NEW) üü° PARTIAL - UI Complete, Backend Pending
   - Status bar display for compaction info implemented (commit a18ced0)
   - Backend logic NOT YET implemented (no ConversationCompactor struct)
   - TODO: Implement actual conversation compaction logic
     - Automatically compact conversation history when it gets too long
     - Summarize older messages to reduce token usage
     - Configurable compaction threshold (e.g., 20k tokens)
     - Uses Claude API to generate summaries
   - Files: `src/conversation/mod.rs`, `src/config/settings.rs`
   - Effort: 1-2 hours remaining (UI done, backend needed)

### Phase 3: Moderate Complexity (3-6 hours each)

13. **[x] Control-C query termination** üî¥ HIGH PRIORITY - ‚úÖ COMPLETE
    - Cancel in-progress queries with Ctrl+C
    - Stops streaming, marks query as cancelled, clears state
    - TODO: Pass cancellation to HTTP requests (abort connections)
    - Files: `src/cli/tui/async_input.rs`, `src/cli/repl_event/events.rs`, `src/cli/repl_event/event_loop.rs`
    - Effort: 2 hours (actual)

14. **[x] Simple response feedback system** (NEW) - ‚úÖ COMPLETE
    - Keyboard shortcuts: Ctrl+G (good), Ctrl+B (bad)
    - Logs to ~/.shammah/feedback.jsonl in JSONL format
    - Weighted feedback (good=1x, bad=10x for LoRA training)
    - Infrastructure complete, needs event loop integration
    - Files: `src/feedback/mod.rs`, `src/cli/tui/async_input.rs`, `src/cli/tui/mod.rs`
    - Effort: 2 hours (actual)

15. **[x] Crash recovery mechanism** (NEW) - ‚úÖ COMPLETE
    - Handle daemon crashes gracefully with auto-restart
    - Detects connection failures and retries queries
    - Auto-restart daemon on connection errors
    - Files: `src/client/daemon_client.rs`, `src/cli/repl_event/event_loop.rs`
    - Effort: 1.5 hours (actual)

16. **[x] Python deps installation helper** (NEW) - ‚úÖ COMPLETE
    - Add `shammah train setup` command
    - Create venv at ~/.shammah/venv
    - Install torch, transformers, peft, safetensors, accelerate
    - Verify installation by importing packages
    - Files: `src/main.rs`
    - Effort: 1.5 hours (actual)

### Phase 4: Challenging (4-8 hours each)

17. **[x] Command history navigation** - ‚úÖ COMPLETE
    - Up/down arrows for previous queries
    - Supports multi-line commands, persists to ~/.shammah/history.txt
    - Loads on startup, saves on shutdown (1000 command limit)
    - Files: `src/cli/tui/async_input.rs`, `src/cli/tui/mod.rs`
    - Effort: 2 hours (actual)

18. **[x] Memory usage monitoring** (NEW) - ‚úÖ COMPLETE
    - Track and display system and process memory usage
    - Show available system RAM, warn if low/critical
    - Files: `src/monitoring/mod.rs`, `src/cli/commands.rs`, `src/cli/repl_event/event_loop.rs`
    - Effort: 1 hour (actual)

19. **[x] Multi-provider setup wizard** - ‚úÖ COMPLETE
    - Support all providers (Claude, GPT-4, Gemini, Grok, Mistral, Groq) in wizard
    - Users can add multiple providers, delete providers, configure API keys
    - Keyboard shortcuts: a (add), d (delete), ‚Üë/‚Üì (select)
    - Files: `src/cli/setup_wizard.rs`
    - Effort: 4 hours (actual)

20. **[x] Tool confirmation system fix** üî¥ CRITICAL SECURITY - ‚úÖ COMPLETE
    - Fixed deadlock issue by integrating dialogs with async input system
    - Dialog key events handled in async_input task (non-blocking)
    - Tool approval dialog shows 6 options (once, session exact/pattern, persistent exact/pattern, deny)
    - Approval responses sent through oneshot channels
    - No more auto-approving of tools
    - Files: `src/cli/tui/mod.rs`, `src/cli/tui/async_input.rs`, `src/cli/repl_event/event_loop.rs`
    - Effort: 5 hours (actual)

21. **[x] Multi-model setup wizard** - ‚úÖ COMPLETE
    - Added Model Preview step showing resolved model details
    - Displays: repository name, parameters, download size, device, RAM requirement
    - Shows exact model that will be downloaded (e.g., "onnx-community/Qwen2.5-3B-Instruct")
    - Helps users understand what they're downloading before committing
    - Keyboard shortcuts: Enter (continue), b (back), Esc (cancel)
    - Files: `src/cli/setup_wizard.rs`
    - Implementation:
      - Added ModelPreview wizard step after ModelSizeSelection
      - Resolves repository based on family + size + device
      - Shows download estimates (3GB-140GB depending on model)
      - Shows RAM requirements (8GB-128GB+ depending on model)
    - Effort: 2 hours (actual)

22. **[~] Mistral model testing** ‚è∏Ô∏è BLOCKED
    - Status: MistralAdapter exists and has passing unit tests
    - Blocker: Only ONNX loader exists, which currently supports Qwen2 only
    - Mistral ONNX models not yet integrated (waiting for onnx-community/Mistral models)
    - Adapter ready at: `src/models/adapters/mistral.rs`
    - Tests passing: format_chat_prompt, clean_output, token_ids
    - Next steps when unblocked:
      1. Wait for onnx-community to publish Mistral ONNX models
      2. Add Mistral support to ONNX loader (similar to Qwen2)
      3. Test end-to-end generation
    - Files: `src/models/adapters/mistral.rs` (ready), `src/models/loaders/onnx.rs` (needs Mistral support)
    - Effort: 2-8 hours (when ONNX models available)

23. **[x] Proxy-only mode** (NEW) üü° MEDIUM PRIORITY - ‚úÖ COMPLETE
    - Allow users to use Shammah without local model (like Claude Code)
    - Daemon still spawns but skips model loading
    - Pure proxy to teacher APIs with tool execution
    - Useful for users who want REPL + tools without model overhead
    - Config option: `[backend] enabled = false`
    - Files: `src/config/backend.rs`, `src/cli/setup_wizard.rs`, `src/main.rs`
    - Implementation:
      - Added `enabled: bool` field to BackendConfig (defaults to true)
      - Added EnableLocalModel wizard step (yes/no choice)
      - If no: skip device/model selection, jump to teacher config
      - If enabled=false in config: skip model loading, set GeneratorState::NotAvailable
      - All queries forwarded to teacher APIs with tool execution
    - Status: ‚úÖ Compiles, wizard flow implemented, integration complete

### Phase 4: Challenging (4-8 hours each) - Continued

24. **[x] Flexible tool approval patterns** (NEW) üî¥ HIGH PRIORITY - ‚úÖ COMPLETE
    - Allow patterns to match command, args, and directory separately
    - Added PatternType::Structured enum variant
    - ToolSignature now includes command, args, directory fields
    - Structured patterns: cmd:"cargo test" args:"*" dir:"/home/*/projects"
    - Support wildcards for each component independently
    - Backward compatible with existing Wildcard and Regex patterns
    - New constructor: ToolPattern::new_structured()
    - Files: `src/tools/patterns.rs`, `src/tools/executor.rs`, `src/cli/repl.rs`
    - Effort: 4 hours (actual)

### Phase 5: Complex (8-20 hours each)

25. **[~] Additional model adapters** (Phi, DeepSeek, etc.) üîÑ IN PROGRESS
    - ‚úÖ Phi adapter complete (Phi-2, Phi-3, Phi-3.5)
      - ChatML-style format with Phi-specific tokens
      - Smart output cleaning (Q&A patterns, role markers)
      - 6/6 tests passing
      - Files: `src/models/adapters/phi.rs` (185 lines)
    - ‚úÖ DeepSeek adapter complete (DeepSeek-Coder, DeepSeek-V2, DeepSeek-V3)
      - Instruction/response format with code block handling
      - Tuned for code generation (temp=0.8, max_tokens=2048)
      - 6/6 tests passing
      - Files: `src/models/adapters/deepseek.rs` (192 lines)
    - üìä Progress: 6 model families now supported (Qwen, Llama, Mistral, Gemma, Phi, DeepSeek)
    - üéØ Remaining: CodeLlama, Yi, StarCoder, or other families (optional)
    - Effort: 3 hours actual (2 models √ó 1.5h each)

26. **[ ] Adapter loading in runtime** üöß COMPLEX
    - Goal: Load trained LoRA adapters during ONNX inference
    - Current state:
      - ‚úÖ Python training script saves adapters to safetensors format
      - ‚úÖ TrainingCoordinator writes JSONL queue
      - ‚úÖ LoRAConfig and infrastructure exist
      - ‚ùå ONNX Runtime is inference-only (no training APIs)
      - ‚ùå No built-in way to apply LoRA weights at runtime
    - Technical challenges:
      1. ONNX Runtime doesn't support dynamic weight modification
      2. Options require significant engineering:
         a. Merge LoRA into base model (requires reloading entire model)
         b. Runtime weight modification (requires low-level ONNX manipulation)
         c. Export merged model as new ONNX (requires PyTorch, 2x memory)
      3. Current Python approach requires loading model twice (memory intensive)
    - Recommended approach:
      - Wait for ONNX Runtime to add training/adapter support
      - Or implement custom Rust LoRA on top of ONNX (40-80 hours)
      - Or use burn.rs with ONNX export (requires rewrite)
    - Files: `src/models/lora.rs`, `scripts/train_lora.py`, `src/models/loaders/onnx.rs`
    - Effort: 40-80 hours (revised from 8-16)

27. **[x] Color scheme customization** (NEW) ‚úÖ COMPLETE
    - Users can now customize TUI colors via config.toml
    - Completed:
      - ‚úÖ Created ColorScheme struct with full color configuration
      - ‚úÖ Defined color categories: Status, Messages, UI, Dialogs
      - ‚úÖ Support for named colors ("cyan", "green") and RGB ([255, 0, 0])
      - ‚úÖ Added to Config struct with Default implementation
      - ‚úÖ TOML serialization/deserialization working
      - ‚úÖ Config loader handles optional colors section
      - ‚úÖ All unit tests passing
      - ‚úÖ Pass ColorScheme to TUI components (TuiRenderer, StatusWidget)
      - ‚úÖ Update StatusWidget::get_line_style() to use scheme colors
      - ‚úÖ Update StatusWidget border color to use scheme.status.border
      - ‚úÖ Integration tested and compiles successfully
    - Future polish (optional):
      - DialogWidget color customization (currently uses defaults)
      - Input widget color customization (currently uses defaults)
      - Message rendering color customization (currently uses defaults)
    - Files:
      - src/config/colors.rs (NEW - 301 lines)
      - src/config/mod.rs (exports ColorScheme types)
      - src/config/settings.rs (Config.colors field)
      - src/config/loader.rs (TOML deserialization)
      - src/cli/tui/mod.rs (TuiRenderer.colors field)
      - src/cli/tui/status_widget.rs (uses ColorScheme)
      - src/cli/repl.rs (passes config.colors to TuiRenderer)
      - ‚úÖ src/config/mod.rs (updated)
      - ‚úÖ src/config/settings.rs (updated)
      - ‚úÖ src/config/loader.rs (updated)
      - ‚è≥ src/cli/tui/status_widget.rs (needs update)
      - ‚è≥ src/cli/tui/dialog_widget.rs (needs update)
      - ‚è≥ src/cli/tui/input_widget.rs (needs update)
    - Effort: 3 hours done, 3-4 hours remaining

### Phase 6: Very Complex (20+ hours)

28. **[x] Plan mode redesign** - ‚úÖ COMPLETE
    - Matches Claude Code's plan mode quality
    - Read-only exploration phase with tool-driven workflow
    - ReplMode enum with Planning/Executing states
    - Tool restrictions enforced (only read, glob, grep, web_fetch allowed in planning)
    - EnterPlanModeTool and PresentPlan implemented
    - Enhanced dialogs with "Other" option
    - Files: `src/cli/repl.rs` (ReplMode enum), `src/tools/implementations/enter_plan_mode.rs`, `src/tools/executor.rs` (line 312)
    - Git commits: b4ef81c, e225859, de2a860, cc049fc, ce2ea8f, 7902ded, ad01b2c
    - Effort: 6-9 hours (actual)

29. **[x] Prompt suggestions** (NEW) ‚úÖ COMPLETE
    - ‚úÖ Full infrastructure with hardcoded + LLM support
    - ‚úÖ SuggestionManager with 7 context-aware states
    - ‚úÖ TUI integration with status bar display
    - ‚úÖ Auto-update on context changes (idle, query complete, error, etc.)
    - ‚úÖ First-run suggestions shown immediately
    - ‚úÖ Documentation: docs/PROMPT_SUGGESTIONS.md
    - Future enhancement: LLM integration in REPL event loop
    - Future enhancement: Clickable suggestions (auto-fill input)
    - Files: `src/cli/suggestions.rs`, `src/cli/tui/mod.rs`, `docs/PROMPT_SUGGESTIONS.md`
    - Effort: 6-12 hours (‚úÖ complete)

30. **[x] LLM-prompted user dialogs** (NEW) ‚úÖ COMPLETE
    - ‚úÖ Full implementation of Claude Code's AskUserQuestion feature
    - ‚úÖ Core data structures (Question, QuestionOption, Input/Output)
    - ‚úÖ Validation logic (question/option counts, header length)
    - ‚úÖ TUI integration (show_llm_question method)
    - ‚úÖ Event loop integration (intercepts before tool execution)
    - ‚úÖ Tool definition added to available tools
    - ‚úÖ Sequential dialog display with answer collection
    - ‚úÖ Proper error handling and cancellation support
    - ‚úÖ Comprehensive documentation (docs/LLM_DIALOGS.md)
    - ‚úÖ Supports single-select and multi-select dialogs (1-4 questions, 2-4 options each)
    - LLM can now prompt user with structured questions during execution
    - Example: "Which library?" with options [Redux, Zustand, Jotai]
    - Files: `src/cli/llm_dialogs.rs`, `src/cli/tui/mod.rs`, `src/cli/repl_event/event_loop.rs`, `src/cli/repl.rs`
    - Effort: 10 hours (actual)

31. **[x] Plan mode toggle with visual indicator** (NEW) ‚úÖ COMPLETE
    - ‚úÖ Shift+Tab keyboard shortcut to toggle plan mode
    - ‚úÖ Status bar indicator shows current mode:
      - Normal: `‚èµ‚èµ accept edits on (shift+tab to cycle)`
      - Plan mode: `‚è∏ plan mode on (shift+tab to cycle)`
    - ‚úÖ `/plan` command (without arguments) toggles mode
    - ‚úÖ Visual feedback when toggling
    - ‚úÖ Updated help text with shortcuts
    - ‚úÖ Provides foundation for full plan mode (Item 26)
    - Does NOT implement plan mode functionality (just UI toggle)
    - Files: `src/cli/tui/mod.rs`, `src/cli/tui/async_input.rs`, `src/cli/commands.rs`, `src/cli/repl_event/event_loop.rs`
    - Effort: 2 hours (actual)

### Phase 7: Documentation (Ongoing)

- [x] Clean up obsolete docs (moved to docs/archive/)
- [x] Update STATUS.md with current capabilities
- [x] Update CLAUDE.md with accurate ONNX architecture and recent progress
- [x] Create user guide (docs/USER_GUIDE.md) - ‚úÖ COMPLETE
- [ ] Update ARCHITECTURE.md with daemon mode

---

## Known Issues

### 1. Persistent Tool Patterns Not Matching
**Issue:** Persistent tool approval patterns not matching subsequent tool calls
**Impact:** Users must re-approve tools that should be permanently allowed
**Security:** Medium priority - affects convenience but doesn't compromise security
**Status:** Reported 2026-02-12
**Fix:** Phase 1 - Debug pattern matching logic (Item 6)

### 2. Plan Mode Needs Redesign
**Issue:** Current plan mode uses manual commands, needs Claude Code-style workflow
**Impact:** Not intuitive, requires manual state management
**Solution:** Tool-driven approach with read-only exploration and approval workflow
**Status:** ‚úÖ Implementation plan complete - See `docs/PLAN_MODE_REDESIGN.md`
**Fix:** Phase 6 - Item 28 (6-9 hours estimated)

### 3. Live Stats Not Showing
**Issue:** Status bar exists but live stats (tokens, latency, model) not populated
**Impact:** Users don't see token counts, speed, model info after queries
**Root cause:** ResponseMetadata fields set to None in generators
**Status:** Diagnosed 2026-02-12
**Fix:** Phase 2 - Implement token counting and latency tracking in generators

### 4. LoRA Training Architecture Limitation
**Issue:** Python-based training inefficient with ONNX Runtime
- ONNX Runtime is inference-only (no training APIs)
- PyTorch training requires loading model twice (2x memory)
- Current solution works but is not optimal
**Impact:** LoRA fine-tuning has high memory overhead
**Fix:** Long-term - Build pure Rust LoRA system
- Option 1: Custom Rust LoRA on top of ONNX Runtime
- Option 2: Use burn.rs with ONNX export
- Option 3: Wait for ONNX Runtime training support
- Option 4: Implement LoRA as ONNX graph modifications

---

## Performance Metrics

**Current Behavior:**
- Local model: Handles queries when ready (ONNX Runtime)
- Graceful fallback: Routes to teacher if model not ready
- Tool execution: Multi-turn loop (max 5 iterations)
- Startup time: <100ms (instant REPL)
- Model loading: 2-3 seconds from cache

**Architecture:**
- Runtime: ONNX Runtime with KV cache
- Models: onnx-community Qwen 1.5B/3B/7B/14B
- Acceleration: Metal (Apple Silicon), CPU fallback
- Daemon: Auto-spawn, OpenAI-compatible API
- Tools: Client-side execution via pass-through

---

## Next Steps (Recommended Order)

### Week 1: Quick Wins for Momentum ‚ö°
1. Daemon stop command (1-2h) - EASIEST FIRST
2. Daemon start command (1-2h)
3. Error message improvements (1-2h)
4. Shift-return support (2-4h)
5. Status bar stats (2-4h)
**Goal:** Build momentum with easy wins, improve UX

### Week 2: Critical Issues üî¥
6. Tool confirmation fix (4-8h) - CRITICAL SECURITY
7. Control-C termination (3-6h) - HIGH PRIORITY
8. Config validation (2-4h)
**Goal:** Fix critical security and UX issues

### Week 3: User Experience Polish
9. Command history (4-8h)
10. Model download progress in TUI (2-4h)
11. /help improvements (2-4h)
12. Daemon status command (2-4h)
**Goal:** Professional REPL experience

### Week 4: Reliability & Setup
13. Crash recovery (4-6h)
14. Simple feedback system (3-6h)
15. Python deps helper (3-5h)
16. Multi-provider wizard (4-6h)
**Goal:** Robust system, easier setup

### Later (As Needed)
- Memory monitoring
- Color customization
- Additional model adapters
- Mistral testing
- Plan mode redesign (big project)

### Long Term (Future)
- **Pure Rust LoRA training system** (high priority)
  - Custom implementation compatible with ONNX Runtime
  - Avoid Python memory overhead (no model duplication)
  - Options: burn.rs, custom ONNX graph mods, or wait for ONNX Training
- Adapter loading in runtime
- Quantization for lower memory usage
- Multi-GPU support
- Custom domain-specific tools
- CoreML export optimization

---

## How to Use

### Basic Usage
```bash
# Interactive REPL mode (daemon auto-spawns)
shammah

> Can you read my Cargo.toml and tell me about dependencies?
```

### Daemon Mode
```bash
# Auto-spawns on first query (no manual start needed)
shammah
> /local what is 2+2?

# Or use HTTP API directly
curl -X POST http://127.0.0.1:8000/v1/messages \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen-2.5-3b", "messages": [{"role": "user", "content": "Hello!"}]}'
```

### Tool Execution
```bash
> Can you read my README.md?
üîß Tool: Read
   File: README.md
   Status: ‚úì Success
[Shows file contents and analysis]
```

---

## Technical Debt

### Code Quality
- [ ] Remove unused Candle imports (migration to ONNX complete)
- [ ] Clean up error messages for better UX
- [ ] Add more comprehensive logging

### Testing
- [ ] Add integration tests for daemon lifecycle
- [ ] Add tests for LoRA training end-to-end
- [ ] Test multi-provider fallback scenarios
- [ ] End-to-end test for tool pass-through

### Documentation
- [x] Update STATUS.md (this file)
- [ ] Update ARCHITECTURE.md with daemon architecture
- [ ] Update CLAUDE.md with ONNX details
- [ ] Create USER_GUIDE.md
- [ ] Document LoRA training workflow

---

## Contributors

- **Human:** Shammah (project vision, requirements)
- **AI:** Claude Sonnet 4.5 (implementation, architecture)

---

## License

MIT OR Apache-2.0
