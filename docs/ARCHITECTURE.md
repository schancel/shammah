# Architecture

This document describes the technical architecture of Shammah, a local-first AI coding assistant that uses pre-trained ONNX models with LoRA fine-tuning.

## Overview

Shammah provides **immediate, high-quality AI assistance** using pre-trained local models (Qwen via ONNX Runtime) or cloud fallback (Claude, GPT-4, Gemini, Grok), then continuously improves through weighted LoRA fine-tuning to adapt to your specific coding patterns.

**Current State (v0.4.0):**
- âœ… ONNX Runtime with KV cache support
- âœ… Pre-trained Qwen models (1.5B/3B/7B/14B)
- âœ… Daemon architecture with auto-spawn
- âœ… OpenAI-compatible HTTP API
- âœ… Tool execution with pass-through
- âœ… SSE streaming for local and remote
- âœ… LoRA training infrastructure (Python-based)
- âœ… Multi-provider teacher support

**Key Innovation:** Pre-trained models + weighted LoRA fine-tuning = immediate quality + continuous improvement.

## Architecture Overview

```
User runs shammah
    â†“
Daemon auto-spawns (if not running)
    â†“
Background: Load ONNX model (if enabled)
    â†“
REPL appears instantly (<100ms)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Router with Model Check             â”‚
â”‚  - Crisis detection (safety)         â”‚
â”‚  - Local model ready? Use local      â”‚
â”‚  - Model loading? Forward to teacher â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    Model Ready?
           â”‚
    â”œâ”€ NO  â†’ Forward to Teacher API (Claude/GPT-4/Gemini/Grok)
    â””â”€ YES â†’ Continue
           â”‚
           v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ONNX Runtime Inference           â”‚
    â”‚ (Qwen 1.5B/3B/7B/14B)           â”‚
    â”‚ + LoRA Adapters (optional)       â”‚
    â”‚ Device: Metal/CPU                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Response to User                â”‚
    â”‚  (Streaming via SSE)             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           v
    User Feedback?
           â”‚
    â”œâ”€ ðŸ”´ Critical issue â†’ High-weight training (10x)
    â”œâ”€ ðŸŸ¡ Could improve â†’ Medium-weight training (3x)
    â””â”€ ðŸŸ¢ Looks good â†’ Normal-weight training (1x)
           â”‚
           v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Background LoRA Fine-Tuning     â”‚
    â”‚  (Python script, non-blocking)   â”‚
    â”‚  - Weighted sampling             â”‚
    â”‚  - Saves to safetensors          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. Daemon Architecture

**Auto-Spawning Daemon:**
- REPL client checks for running daemon (PID file at `~/.shammah/daemon.pid`)
- If not running, spawns daemon process automatically
- Health checks ensure daemon is responsive
- Graceful restart on crashes

**OpenAI-Compatible HTTP API:**
- Port 11435 (11434 is used by Ollama)
- Endpoint: `POST /v1/chat/completions`
- Drop-in replacement for OpenAI/Claude clients
- Session management with concurrent client support

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REPL Client (tui-based)           â”‚
â”‚   - Keyboard input handling         â”‚
â”‚   - Streaming UI rendering          â”‚
â”‚   - Tool confirmation dialogs       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ HTTP (port 11435)
            v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Auto-Spawned Daemon               â”‚
â”‚   - PID management                  â”‚
â”‚   - Health monitoring               â”‚
â”‚   - Session cleanup                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      v           v         v         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ ONNX    â”‚ â”‚ Teacher â”‚ â”‚ Tool â”‚ â”‚ LoRA â”‚
â”‚ Runtime â”‚ â”‚ APIs    â”‚ â”‚ Exec â”‚ â”‚Train â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

**Key Files:**
- `src/daemon/server.rs` - Axum HTTP server
- `src/daemon/lifecycle.rs` - PID management, auto-spawn
- `src/client/daemon_client.rs` - HTTP client with health checks

### 2. ONNX Model Integration

**Purpose:** Load and run pre-trained models with KV cache for efficient autoregressive generation.

**Model Selection (RAM-based):**
- 8GB Mac â†’ Qwen-2.5-1.5B (1.5GB RAM, fast)
- 16GB Mac â†’ Qwen-2.5-3B (3GB RAM, balanced)
- 32GB Mac â†’ Qwen-2.5-7B (7GB RAM, powerful)
- 64GB+ Mac â†’ Qwen-2.5-14B (14GB RAM, maximum)

**Features:**
- ONNX Runtime with CoreML execution provider (Metal acceleration on Apple Silicon)
- Full KV cache support (56+ dynamic inputs for 28 layers)
- Autoregressive generation with cache reuse
- Graceful CPU fallback
- Automatic tokenizer loading from HuggingFace Hub

**KV Cache Architecture:**
```rust
// Empty cache initialization (shape: [1, 2, 0, 128])
let mut kv_cache: Vec<Array4<f32>> = Vec::new();
for layer in 0..28 {
    kv_cache.push(Array4::zeros((1, 2, 0, 128))); // K and V
}

// Each generation step:
1. Bind input_ids, attention_mask, position_ids
2. Bind 56 KV cache inputs (28 layers Ã— 2)
3. Run inference
4. Extract logits and updated KV cache
5. Reuse updated cache for next token
```

**Key Files:**
- `src/models/loaders/onnx.rs` - OnnxLoader, KV cache management
- `src/generators/qwen.rs` - QwenGenerator with multi-turn execution
- `src/models/adapters/qwen.rs` - Output cleaning and prompt formatting

### 3. Tool Execution System

**Purpose:** Enable AI to inspect and modify code through structured tools.

**Available Tools:**
- **Read** - Read file contents (code, configs, docs)
- **Glob** - Find files by pattern (`**/*.rs`)
- **Grep** - Search with regex (`TODO.*`)
- **WebFetch** - Fetch URLs (documentation, examples)
- **Bash** - Execute commands (tests, build, etc.)
- **Restart** - Self-improvement (modify code, rebuild, restart)

**Tool Pass-Through Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REPL Client    â”‚
â”‚  (runs locally) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1. Send query
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Daemon Server  â”‚
â”‚  (model, API)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 2. Returns tool_use blocks
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REPL Client    â”‚
â”‚  - Executes toolâ”‚  â† Client has filesystem access
â”‚  - Shows dialog â”‚  â† Client owns terminal UI
â”‚  - Collects     â”‚
â”‚    results      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 3. Send tool results
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Daemon Server  â”‚
â”‚  (final response)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Pass-Through?**
- Client has filesystem access (daemon may not)
- Client owns terminal UI for confirmation dialogs
- Proper security model (user approves tools on their machine)
- Simple: daemon is stateless for tool execution

**Multi-Turn Loop:**
```
User Query â†’ Daemon (with tool definitions)
    â†“
Model returns tool_use blocks
    â†“
Client executes tools â†’ collects results
    â†“
Send results back to daemon (maintain conversation)
    â†“
Model returns final response (or more tool uses)
    â†“
Repeat up to 5 iterations
```

**Key Files:**
- `src/tools/executor.rs` - ToolExecutor, multi-turn loop
- `src/tools/implementations/` - Individual tool implementations
- `src/tools/permissions.rs` - PermissionManager, approval patterns
- `src/cli/repl_event/tool_execution.rs` - Client-side execution

### 4. SSE Streaming Implementation

**Purpose:** Provide real-time token-by-token response streaming for both local and remote models.

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generator (Local or Remote)        â”‚
â”‚  - Token-by-token generation        â”‚
â”‚  - Callbacks on each token          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SSE Event Stream                   â”‚
â”‚  - Server-Sent Events format        â”‚
â”‚  - Bounded channel (size 2)         â”‚
â”‚  - Natural backpressure             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TUI Renderer                       â”‚
â”‚  - Real-time UI updates (20 FPS)    â”‚
â”‚  - Shadow buffer diff rendering     â”‚
â”‚  - Scrollback integration           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- Prevents connection timeouts on long queries (>10s)
- Responsive UI showing generation progress
- Cancel queries mid-generation (Ctrl+C)
- Works with both local ONNX and remote APIs

**Key Files:**
- `src/daemon/streaming.rs` - SSE event formatting
- `src/cli/tui/mod.rs` - TUI streaming response handling
- `src/generators/qwen.rs` - Token callbacks

### 5. LoRA Fine-Tuning Infrastructure

**Purpose:** Efficient domain-specific adaptation with weighted examples.

**Architecture:**
```
User Feedback (10x/3x/1x weight)
    â†“
TrainingCoordinator collects examples
    â†“
Write to JSONL queue (~/.shammah/training_queue.jsonl)
    â†“
Spawn Python training script (background)
    â†“
PyTorch + PEFT LoRA training
    â†“
Save adapter to safetensors format
    â†“
(Future) Load adapter in ONNX Runtime
```

**Weighted Training:**
- **High-weight (10x)**: Critical issues (strategy errors)
  - Example: "Never use .unwrap() in production"
  - Impact: Model strongly learns to avoid this
- **Medium-weight (3x)**: Style preferences
  - Example: "Prefer iterator chains over manual loops"
  - Impact: Model learns your preferred approach
- **Normal-weight (1x)**: Good examples
  - Example: "This is exactly right"
  - Impact: Model learns normally

**Current Limitation:**
ONNX Runtime is inference-only (no training APIs). Adapters are trained in Python but not yet loaded at runtime. Future work: implement adapter loading via weight merging or custom ONNX graph modifications.

**Key Files:**
- `src/models/lora.rs` - WeightedExample, TrainingCoordinator
- `src/training/lora_subprocess.rs` - Python subprocess spawner
- `scripts/train_lora.py` - PyTorch LoRA training script

### 6. TUI Renderer System

**Purpose:** Professional terminal UI with scrollback, streaming, and efficient updates.

**Dual-Layer Architecture:**

1. **Terminal Scrollback** (permanent, scrollable with Shift+PgUp)
   - Written via `insert_before()` for new messages
   - Pushes content above the inline viewport
   - Preserves full history (scrollable by user)

2. **Inline Viewport** (6 lines at bottom, double-buffered)
   - Separator line (visual boundary)
   - Input area (4 lines, tui-textarea)
   - Status bar (1 line, model/token info)

**Key Innovation: Immediate Scrollback with Efficient Updates**
```
New message â†’ Write to scrollback immediately via insert_before()
Message updates â†’ Diff-based blitting to visible area only
```

**Shadow Buffer System:**
- 2D char array with proper text wrapping
- ANSI escape code preservation (zero-width)
- Diff-based rendering (only changed cells)
- Bottom-aligned content (recent messages visible)

**Key Files:**
- `src/cli/tui/mod.rs` - TuiRenderer, flush_output_safe(), blit_visible_area()
- `src/cli/tui/shadow_buffer.rs` - ShadowBuffer, diff_buffers()
- `src/cli/tui/scrollback.rs` - ScrollbackBuffer (message tracking)
- `src/cli/tui/input_widget.rs` - Input area rendering
- `src/cli/tui/status_widget.rs` - Status bar rendering

### 7. Multi-Provider Teacher Support

**Purpose:** Flexible fallback to multiple cloud AI providers.

**Supported Providers:**
- **Claude** (Anthropic) - Primary, full capability
- **GPT-4** (OpenAI) - Full capability
- **Gemini** (Google) - Full capability
- **Grok** (xAI) - Full capability
- **Mistral** (Mistral AI) - Full capability
- **Groq** (Groq) - Fast inference

**Provider Adapters:**
Each provider has an adapter that handles:
- API request formatting (convert to provider's schema)
- Tool definition translation
- Response parsing
- Capability mapping (streaming, tool use, etc.)

**Adaptive Routing:**
```
1. Try local model if ready
2. On failure/unavailable, try first teacher
3. On API error, try next teacher in priority list
4. Graceful degradation ensures user always gets response
```

**Key Files:**
- `src/providers/` - Provider-specific adapters
- `src/config/settings.rs` - TeacherEntry configuration
- `src/cli/setup_wizard.rs` - Multi-provider setup UI

### 8. Conversation Management

**Purpose:** Manage multi-turn conversation history with context window limits.

**Features:**
- Automatic message trimming (last 20 messages)
- Token-based limits (~8K tokens per conversation)
- Session persistence (save/restore)
- Conversation compaction (auto-summarization)

**Compaction Architecture:**
```
Conversation grows â†’ 80% of max tokens
    â†“
Trigger auto-compaction
    â†“
Summarize older messages (keep recent intact)
    â†“
Replace old messages with summary
    â†“
Continue conversation with reduced token count
```

**Key Files:**
- `src/cli/conversation.rs` - ConversationHistory
- `src/conversation/compactor.rs` - ConversationCompactor (to be implemented)

## System Flow

### REPL Session Flow

```
1. User starts `shammah`
2. Check for running daemon (PID file)
3. If not running, spawn daemon process
4. Wait for daemon health check (up to 5s)
5. Display TUI with empty prompt
6. Background: Daemon loads ONNX model (if enabled)
7. User types query
8. Send HTTP POST to daemon
9. Daemon routes query (local or teacher)
10. Stream response tokens back to client (SSE)
11. TUI renders tokens in real-time (20 FPS)
12. If tool_use blocks, execute on client side
13. Send tool results back to daemon
14. Repeat until final response
15. Log metrics and feedback
```

### Daemon Lifecycle

```
1. Daemon starts (via auto-spawn or manual)
2. Load ONNX model (if backend enabled)
   - Download from HuggingFace Hub (first run)
   - Initialize KV cache
   - Load LoRA adapter (if exists)
3. Start HTTP server (port 11435)
4. Write PID file (~/.shammah/daemon.pid)
5. Accept client connections
6. Handle queries concurrently
7. On SIGTERM/SIGINT, gracefully shutdown
8. Clean up sessions and resources
9. Remove PID file
```

## Data Flow

### Request Processing

```
1. Receive user query (HTTP POST)
2. Extract session_id (or create new session)
3. Add to conversation history
4. Router decision: local vs. teacher
5. If local:
     a. ONNX inference â†’ local response
     b. Return response
6. If teacher:
     a. Forward to teacher API (Claude/GPT-4/etc.)
     b. Parse tool_use blocks
     c. Return tool_use to client
     d. Client executes tools
     e. Client sends tool results
     f. Forward tool results to teacher
     g. Return final response
7. Log metrics (routing, latency, tokens)
8. Update session last_activity
```

### Metrics Collection

Every request logs:
```json
{
  "timestamp": "2026-02-14T12:00:00Z",
  "session_id": "abc123",
  "routing_decision": "local",
  "response_time_ms": 650,
  "tokens_generated": 127,
  "tool_uses": 2,
  "forward_reason": null
}
```

Stored in: `~/.shammah/metrics/YYYY-MM-DD.jsonl`

### Training Data Format

```json
{
  "id": "uuid",
  "timestamp": "2026-02-14T12:00:00Z",
  "query": "What is the golden rule?",
  "response": "The golden rule refers to...",
  "feedback_weight": 1.0,
  "feedback_type": "normal",
  "used_for_training": true
}
```

Stored in: `~/.shammah/training_queue.jsonl`

## File Structure

```
~/.shammah/
â”œâ”€â”€ config.toml              # User configuration
â”œâ”€â”€ daemon.pid               # Daemon process ID
â”œâ”€â”€ daemon.sock              # IPC socket (unused, HTTP preferred)
â”œâ”€â”€ adapters/                # LoRA adapters
â”‚   â”œâ”€â”€ coding_2026-02-06.safetensors
â”‚   â””â”€â”€ rust_advanced.safetensors
â”œâ”€â”€ metrics/                 # Daily JSONL logs
â”‚   â””â”€â”€ 2026-02-14.jsonl
â”œâ”€â”€ training_queue.jsonl     # Pending training examples
â””â”€â”€ tool_patterns.json       # Approved tool patterns

~/.cache/huggingface/hub/    # Base models (HF standard)
â”œâ”€â”€ models--onnx-community--Qwen2.5-1.5B-Instruct/
â”œâ”€â”€ models--onnx-community--Qwen2.5-3B-Instruct/
â””â”€â”€ models--onnx-community--Qwen2.5-7B-Instruct/
```

## Technology Stack

**Language:** Rust 2021 edition
- Memory safety without GC
- High performance
- Excellent Apple Silicon support

**ML Framework:** ONNX Runtime
- Cross-platform inference engine
- CoreML execution provider for Apple Silicon (Metal acceleration)
- CPU fallback for maximum compatibility
- KV cache support for efficient autoregressive generation
- ONNX format (optimized, portable)

**Models:**
- Base: Qwen-2.5-1.5B/3B/7B/14B (ONNX format, pre-trained)
- Source: onnx-community on HuggingFace
- Adapters: LoRA (domain-specific, ~5MB each, via Python training)

**HTTP Server:** Axum
- Tokio async runtime
- Tower middleware stack
- Efficient request routing

**TUI:** Ratatui
- Modern terminal UI framework
- Composable widgets
- Efficient rendering

**Dependencies:**
- `ort` - ONNX Runtime bindings (Rust)
- `hf-hub` - HuggingFace Hub integration
- `tokenizers` - Tokenization (HF tokenizers crate)
- `tokio` - Async runtime
- `axum` - HTTP server
- `ratatui` - TUI framework
- `sysinfo` - System RAM detection

## Performance Targets

### Current Performance (v0.4.0)

**Startup:**
- REPL available: <100ms (instant)
- Daemon spawn: 2-3s from cache
- Model loading: 5-10s background (non-blocking)

**Response Time:**
- Local generation: 500ms-2s (depending on model size)
- With LoRA adapter: +50-100ms overhead
- Teacher API: Standard API latency (1-3s)
- Tool execution: ~50-200ms per tool

**Resource Usage:**
- RAM: 3-28GB (depending on model size)
- Disk: 1.5-14GB for base model + ~5MB per adapter
- CPU (idle): <5%

**Daemon:**
- Throughput: 1000+ requests/second (health checks)
- Latency overhead: <5ms (excluding model inference)
- Memory per session: ~20MB
- Max concurrent sessions: 100 (configurable)

## Security & Privacy

### Data Protection
- All metrics hashed (SHA256) for privacy
- Models train only on YOUR data
- No telemetry, no cloud sync
- Can delete `~/.shammah/` anytime

### Tool Safety
- Permission system with approval dialogs
- Session and persistent patterns
- Wildcard and regex matching
- User controls all tool execution

### Daemon Security
- Binds to localhost by default (127.0.0.1)
- API key authentication (Phase 4 - not yet implemented)
- Rate limiting (Phase 4 - not yet implemented)
- TLS support via reverse proxy (nginx)

**Current Recommendations:**
- Only bind to localhost unless on trusted network
- Use firewall rules to restrict access
- Run behind reverse proxy (nginx) for production
- Monitor logs for suspicious activity

## Future Optimizations

### Pure Rust LoRA Training
- Current: Python-based training (2x memory overhead)
- Future: Custom Rust implementation compatible with ONNX Runtime
- Options: burn.rs, custom ONNX graph mods, or wait for ONNX Training support

### Adapter Loading at Runtime
- Load trained LoRA adapters during ONNX inference
- Requires weight merging or dynamic ONNX graph modification
- Enables instant domain switching without reloading base model

### Quantization
- INT8 quantization for lower memory usage
- Faster inference on Neural Engine
- Trade-off: slight quality reduction for 4x memory savings

### Multi-GPU Support
- Distribute inference across multiple GPUs
- Enables larger models (70B+)
- Requires model parallelism implementation

## References

- **DAEMON_MODE.md** - Detailed daemon architecture and API
- **TOOL_CONFIRMATION.md** - Tool permission system details
- **TUI_ARCHITECTURE.md** - Terminal UI rendering system
- **USER_GUIDE.md** - Setup and usage instructions
- **ROADMAP.md** - Future work planning

---

**Current Version:** 0.4.0 (Production-Ready with Tool Confirmations)
**Last Updated:** 2026-02-14
